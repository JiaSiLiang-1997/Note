# 基本概念

**MTU**

泛指通讯协议中的最大传输单元。一般用来说明TCP/IP四层协议中数据链路层的最大传输单元，不同类型的网络MTU也会不同，我们普遍使用的以太网的MTU是1500，即最大只能传输1500字节的数据帧。可以通过ifconfig命令查看电脑各个网卡的MTU。

**MSS**

指TCP建立连接后双方约定的可传输的最大TCP报文长度，是TCP用来限制应用层可发送的最大字节数。如果底层的MTU是1500byte，则 MSS = 1500- 20(IP Header) -20 (TCP Header) = 1460 byte。



# TCP报文结构

![v2-d74e7536e6c0b8e2cbd0e5810e5b31e0_720w](https://jsl1997.oss-cn-beijing.aliyuncs.com/note/v2-d74e7536e6c0b8e2cbd0e5810e5b31e0_720w.jpg)

上图是TCP报文结构，以下是首部各个字段的描述：

- 占用16位的**源端口**号和**目的端口**号，标识了发送方和接收方的进程标识；
- 32位的**序号**字段表示数据报文的编号，应用层要发送一段应用数据，当传送到传输层后，如果应用数据大小超过MSS（最大报文段长度）时，就会对应用层数据字节流从头到尾按顺序进行编号，然后再切分为一个个MSS大小的TCP报文段，每个报文段的序号字段就是该分段的第一个字节的序号；
- 32位的**确认序号**字段表示希望对端发送的下一个报文段的序号。由于TCP是一种全双工的通讯协议，即发送方既发送数据也可能接收对端发送的数据，如源主机发送0-1000编号的字节段给目的主机，这时目的主机就会发送一个响应报文且确认序号为1001，表示希望收到下一个报文段编号为1001的数据，它和序号字段共同保证了TCP报文的可靠性；
- 4位的**首部长度**表示TCP报文首部的长度，该字段以4字节为单位。TCP首部的长度是可变的，在特殊情况下选项字段会有值，也计入首部长度，但一般情况下选项都会为空，这样首部长度就是20个字节，因此该字段的数值就为5，用4位二进制表示就是0101；
- 6位**保留位**未用到，可能以后TCP版本升级会被征用；
- 之后的6位是**标志位字段**，其中，SYN用于TCP连接的建立，FIN用于连接的关闭，RST是重新连接，ACK表示对发送方数据的确认，和确认序号结合使用；PSH标识⽴刻将数据交给上层处理；URG标识数据中需要被上层处理的紧急数据，而紧急数据部分是通过**紧急指针**字段来标识哪一段属于紧急数据；
- 16位**窗口大小**用于标识自己能接受的最大报文数，用于控制TCP连接的数据流量；
- 16位**校验和字段**用于接收端验证数据包的准确性，防止中途被篡改，这也是保证了TCP的可靠性。

# TCP 三次握手过程

如下图所示，下面的两个机器人通过 3 次握手确定了对方能正确接收和发送消息(图片来源：《图解 HTTP》)。

![三次握手](https://jsl1997.oss-cn-beijing.aliyuncs.com/note/三次握手.png)

**简单示意图：**

![三次握手2](https://jsl1997.oss-cn-beijing.aliyuncs.com/note/三次握手2.png)

- 客户端–发送带有 SYN 标志的数据包 – 一次握手 – 服务端
- 服务端–发送带有 SYN/ACK 标志的数据包 – 二次握手 – 客户端
- 客户端–发送带有带有 ACK 标志的数据包 – 三次握手 – 服务端

**详细示意图（图片来源不详）**

![0c9f470819684156cfdc27c682db4def](https://jsl1997.oss-cn-beijing.aliyuncs.com/note/0c9f470819684156cfdc27c682db4def.png)

> 最开始的时候客户端和服务器都是处于CLOSED状态。主动打开连接的为客户端，被动打开连接的是服务器。

1. TCP服务器进程先创建传输控制块TCB，时刻准备接受客户进程的连接请求，此时服务器就进入了LISTEN（监听）状态；
2. TCP客户进程也是先创建传输控制块TCB，然后向服务器发出连接请求报文，这是报文首部中的同部位SYN=1，同时选择一个初始序列号 seq=x ，此时，TCP客户端进程进入了 SYN-SENT（同步已发送状态）状态。==TCP规定，SYN报文段（SYN=1的报文段）不能携带数据，但需要消耗掉一个序号。==
3. TCP服务器收到请求报文后，如果同意连接，则发出确认报文。确认报文中应该 ACK=1，SYN=1，确认号是ack=x+1，同时也要为自己初始化一个序列号 seq=y，此时，TCP服务器进程进入了SYN-RCVD（同步收到）状态。==这个报文也不能携带数据，但是同样要消耗一个序号。==
4. TCP客户进程收到确认后，还要向服务器给出确认。确认报文的ACK=1，ack=y+1，自己的序列号seq=x+1，此时，TCP连接建立，客户端进入ESTABLISHED（已建立连接）状态。==TCP规定，ACK报文段可以携带数据，但是如果不携带数据则不消耗序号。==
5. 当服务器收到客户端的确认后也进入ESTABLISHED状态，此后双方就可以开始通信了。
   

# 为什么要三次握手，为什么不是两次、四次？

**三次握手的目的是建立可靠的通信信道，说到通讯，简单来说就是数据的发送与接收，而三次握手最主要的目的就是双方确认自己与对方的发送与接收是正常的。**

第一次握手：Client 什么都不能确认；Server 确认了对方发送正常，自己接收正常

第二次握手：Client 确认了：自己发送、接收正常，对方发送、接收正常；Server 确认了：对方发送正常，自己接收正常

第三次握手：Client 确认了：自己发送、接收正常，对方发送、接收正常；Server 确认了：自己发送、接收正常，对方发送、接收正常

所以三次握手就能确认双发收发功能都正常，缺一不可。（所以不需要四次，四次就浪费资源了）

# 为什么SYN/FIN不包含数据却要消耗一个序列号？ACK不需要？

**凡是需要对端确认的，一定要消耗TCP报文的序列号。**不然可能会因为网络的原因，进行了重发，比如SYN请求发了两、三次，如果没有序列号，那么服务端做的处理可能会造成资源的浪费，会对重发的数据进行重复处理。

SYN和FIN需要对端的确认，所以需要消耗一个序列号。

ACK并不需要对端确认，所以不需要消耗一个序列号。

**原因是 SYN 和 FIN 信号都是需要 acknowledgement 的，也就是你必须回复这个信号，如果它不占有一个字节的话，要如何判断你是回复这个信号还是回复这个信号之前的包呢？**

例如：如果 FIN 信号不占用一个字节，回复 FIN 的 ack 包就可能被误认为是回复之前的数据包被重新发送了一次，第二次挥手无法完成，连接也就无法正常关闭了。



# 什么是半连接队列？什么是SYN Flood攻击？

客户端大量伪造IP发送SYN包，服务端回复的ACK+SYN去到了一个「未知」的IP地址，势必会造成服务端大量的连接处于SYN_RCVD 状态，而服务器的半连接队列大小也是有限的，如果半连接队列满，也会出现无法处理正常请求的情况。

![image-20220221203245066](https://jsl1997.oss-cn-beijing.aliyuncs.com/note/image-20220221203245066.png)

**防范措施：**

- **无效连接监视释放：**这种方法不停监视系统的半开连接和不活动连接，当达到一定阈值时拆除这些连接，从而释放系统资源。这种方法对于所有的连接一视同仁，而且由于SYN Flood造成的半开连接数量很大，正常连接请求也被淹没在其中被这种方式误释放掉，因此这种方法属于入门级的SYN Flood方法。
- **延缓TCB分配方法：**从前面SYN Flood原理可以看到，消耗服务器资源主要是因为当SYN数据报文一到达，系统立即分配TCB，从而占用了资源。而SYN Flood由于很难建立起正常连接，因此，当正常连接建立起来后再分配TCB则可以有效地减轻服务器资源的消耗。常见的方法是使用SYN Cache和SYN Cookie技术。
  - **SYN Cache技术：**这种技术是在收到SYN数据报文时不急于去分配TCB，而是先回应一个SYN ACK报文，并在一个专用HASH表（Cache）中保存这种半开连接信息，直到收到正确的回应ACK报文再分配TCB。
  - **SYN Cookie技术：**对于SYN攻击，SYN Cache虽然不分配TCB，但是为了判断后续对方发来的ACK报文中的Sequence Number的正确性，还是需要使用一些空间去保存己方生成的Sequence Number等信息，也造成了一些资源的浪费。Syn Cookie技术则完全不使用任何存储资源，这种方法比较巧妙，它使用一种特殊的算法生成Sequence Number，这种算法考虑到了对方的IP、端口、己方IP、端口的固定信息，以及对方无法知道而己方比较固定的一些信息，如MSS、时间等，在收到对方的ACK报文后，重新计算一遍，看其是否与对方回应报文中的（SequenceNumber-1）相同，从而决定是否分配TCB资源。

- **SYN Proxy防火墙：**SYN Cache技术和SYN Cookie技术总的来说是一种主机保护技术，需要系统的TCP/IP协议栈的支持，而目前并非所有的操作系统支持这些技术。因此很多防火墙中都提供一种 SYN代理的功能，其主要原理是对试图穿越的SYN请求进行验证后才放行。





# TCP四次挥手过程

![image-20220220220445767](https://jsl1997.oss-cn-beijing.aliyuncs.com/note/image-20220220220445767.png)数据传输完毕后，双方都可释放连接。最开始的时候，客户端和服务器都是处于ESTABLISHED状态，然后客户端主动关闭，服务器被动关闭。

1. 客户端进程发出连接释放报文，并且停止发送数据。释放数据报文首部，FIN=1，其序列号为seq=u（等于前面已经传送过来的数据的最后一个字节的序号加1），此时，客户端进入FIN-WAIT-1（终止等待1）状态。 ==TCP规定，FIN报文段即使不携带数据，也要消耗一个序号。==
2. 服务器收到连接释放报文，发出确认报文，ACK=1，ack=u+1，并且带上自己的序列号seq=v，此时，服务端就进入了CLOSE-WAIT（关闭等待）状态。==TCP服务器通知高层的应用进程，客户端向服务器的方向就释放了，这时候处于半关闭状态，即客户端已经没有数据要发送了，但是服务器若发送数据，客户端依然要接受。这个状态还要持续一段时间，也就是整个CLOSE-WAIT状态持续的时间。==
3. 客户端收到服务器的确认请求后，此时，客户端就进入FIN-WAIT-2（终止等待2）状态，等待服务器发送连接释放报文==（在这之前还需要接受服务器发送的最后的数据）==。
4. 服务器将最后的数据发送完毕后，就向客户端发送连接释放报文，FIN=1，ack=u+1，由于在半关闭状态，服务器很可能又发送了一些数据，假定此时的序列号为seq=w，此时，服务器就进入了LAST-ACK（最后确认）状态，等待客户端的确认。
5. 客户端收到服务器的连接释放报文后，必须发出确认，ACK=1，ack=w+1，而自己的序列号是seq=u+1，此时，客户端就进入了TIME-WAIT（时间等待）状态。==注意此时TCP连接还没有释放，必须经过2 *MSL（最长报文段寿命）的时间后，当客户端撤销相应的TCB后，才进入CLOSED状态。==
6. 服务器只要收到了客户端发出的确认，立即进入CLOSED状态。同样，撤销TCB后，就结束了这次的TCP连接。==可以看到，服务器结束TCP连接的时间要比客户端早一些。==

# TCP的四次挥手为什么是四次？为什么不能是三次？

假如：第二次和第三次合在一起，当客户端发起断开连接请求，那么服务端必定会有ACK回复，并且，如果服务端还有数据未发送完毕，则也应该发送给客户端。但是，==现实情况是，ACK回复和数据发送不一定是在同一时间发送出去，可能存在一定的延时，因此，就不能合在一起发送，因为，如果客户端没有及时收到ACK回复，就会引发超时重传，更加浪费资源。==

任何一方都可以在数据传送结束后发出连接释放的通知，待对方确认后进入半关闭状态。当另一方也没有数据再发送的时候，则发出连接释放通知，对方确认后就完全关闭了 TCP 连接。

举个例子：A 和 B 打电话，通话即将结束后，A 说“我没啥要说的了”，B 回答“我知道了”，但是 B 可能还会有要说的话，A 不能要求 B 跟着自己的节奏结束通话，于是 B 可能又巴拉巴拉说了一通，最后 B 说“我说完了”，A 回答“知道了”，这样通话才算结束。

# TCP 有哪几种关闭的情况？

参考文章：https://www.cnblogs.com/549294286/p/5077237.html

**客户端关闭连接：**

- 客户端调用close()
- 客户端进程关闭
- 客户端调用shutdown()
- 客户端调用close() + SO_LINGER选项
- 客户端崩溃

**服务器关闭连接：**

- 服务器调用close()
- 服务器进程关闭
- 服务器崩溃
- 服务器崩溃 + SO_KEEPALIVE选项

# 说说TCP快速打开（TFO）的原理

TCP快速打开(TCP Fast Open, TFO)
TFO是在原来TCP协议上的扩展协议，它的主要原理就在发送第一个SYN包的时候就开始传数据了，不过它要求当前客户端之前已经完成过「正常」的三次握手。
快速打开分两个阶段：请求Fast Open Cookie和真正开始TCP Fast Open
**Fast Open Cookie**

![image-20220221210007258](https://jsl1997.oss-cn-beijing.aliyuncs.com/note/image-20220221210007258.png)

**TCP Fast Open**

![image-20220221210036884](https://jsl1997.oss-cn-beijing.aliyuncs.com/note/image-20220221210036884.png)

**TCP Fast Open的优势**

- 一个最显著的优点是可以利用握手去除一个往返 RTT，也可以防止SYN-Flood攻击之类的

![image-20220221210359497](https://jsl1997.oss-cn-beijing.aliyuncs.com/note/image-20220221210359497.png)

# TCP报文中的时间戳有什么作用？

![image-20220221210738444](https://jsl1997.oss-cn-beijing.aliyuncs.com/note/image-20220221210738444.png)

![image-20220221210820540](https://jsl1997.oss-cn-beijing.aliyuncs.com/note/image-20220221210820540.png)

TCP Timestamps Option由四部分构成：
类别(kind)、长度(Length)、 发送方时间戳 (TS value)、回显时间戳(TS Echo Reply)

TCP的时间戳主要解决两大问题：

- **计算往返时延RTT（Round-Trip Time）**

![image-20220221211039652](https://jsl1997.oss-cn-beijing.aliyuncs.com/note/image-20220221211039652.png)

在启用Timestamps选项以后，因为ACK包里包含了TSval和TSecr，这样无论是正常确认包，还是重传确认包，都可以通过这两个值计算出RTT。

- **防止序列号的回绕问题**

TCP的序列号用32bit来表示，因此在2^32字节的数据传输后序列号就会溢出回绕。TCP的窗口经过窗口缩放可以最高到1GB (2^30)， 在高速网络中，序列号在很短的时间内就会被重复使用。

![image-20220221211851222](https://jsl1997.oss-cn-beijing.aliyuncs.com/note/image-20220221211851222.png)

<img src="https://jsl1997.oss-cn-beijing.aliyuncs.com/note/image-20220221212301993.png" alt="image-20220221212301993" style="zoom:67%;" />

假设发送了6个数据包，每个数据包的大小为1GB，第5个包序列号发生回绕。
第2个包因为某些原因延迟导致重传，但没有丢失到时间t7才到达。
这个迷途数据包与后面要发送的第6个包序列号完全相同，如果没有一些措施进行区分，将会造成数据的紊乱。
有Timestamps的存在，迷途数据包与第6个包可以区分。

# TCP的超时重传时间是如何计算的？

TCP具有超时重传机制，即间隔一段时间没有等到数据包的回复时，重传这个数据包。这个重传间隔也叫做**超时重传时间**(Retransmission TimeOut，简称RTO)

![image-20220221213817466](https://jsl1997.oss-cn-beijing.aliyuncs.com/note/image-20220221213817466.png)

**经典方法(适用RTT波动较小的情况)**
往返时延RTT(Round-Trip Time)
一个最简单的想法就是取平均值，比如第一次RTT为500ms，第二次RTT为800ms，那么第三E次发送时，各让一步取平均值RTO为650ms。

经典算法引入了「平滑往返时间」(Smoothed round trip time，SRTT) ：经过平滑后的RTT的值，每测量一次RTT就对SRTT作一次更新计算。

 ![image-20220221214520554](https://jsl1997.oss-cn-beijing.aliyuncs.com/note/image-20220221214520554.png)

α是平滑因子，建议值是0.8 ~ 0.9

假设平滑因子 α = 0.8

SRTT = 80%的原始值 + 20%的新采样RTT值

当α趋近于1时: SRTT越接近上一次的SRTT值，与新的RTT值的关系越小，表现出来就是对短暂的时延变化越不敏感。

当α趋近于0时，1 - α趋近于1，SRTT越接近新采样的RTT值，与旧的SRTT值关系越小，表现出来就是其实容就跟我们采的值有关系延的变化而变化

# 能不能说一说TCP的流量控制？

对于发送端和接收端而言，TCP需要把发送的数据放到发送缓存区,将接收的数据放到接收缓存区。
而流量控制要做的事情，就是在通过接收缓存区的大小，控制发送端的发送。如果对方的接收缓存区满了，就不能再继续发送了。
为了控制发送端的速率，接收端会告知客户端自己接收窗口(rwnd) ，也就是接收缓冲区中空闲的部分。

![image-20220221223627223](https://jsl1997.oss-cn-beijing.aliyuncs.com/note/image-20220221223627223.png)

![image-20220221230143358](https://jsl1997.oss-cn-beijing.aliyuncs.com/note/image-20220221230143358.png)

**发送端的数据包的状态**

- 已发送且已确认；
- 已发送但未确认；
- 未发送但接收端可以接收（接收端有空间接收）
- 未发送且不可以发送（接收端没空间接收）

![image-20220221230642020](https://jsl1997.oss-cn-beijing.aliyuncs.com/note/image-20220221230642020.png)

**发送端速度比较慢的情况**

![image-20220221230857601](https://jsl1997.oss-cn-beijing.aliyuncs.com/note/image-20220221230857601.png)

**发送端速度比较快的情况**

![image-20220221231029331](https://jsl1997.oss-cn-beijing.aliyuncs.com/note/image-20220221231029331.png)

# TCP 中常见的拥塞控制算法有哪些？具体是怎么实现的？

在某段时间，若**对网络中某一资源的需求超过了该资源所能提供的可用部分，网络性能就要变坏**，这种情况就叫做**网络拥塞**。

在计算机网络中数位链路容量（即带宽）、交换结点中的缓存和处理机等，都是网络的资源。

若**出现拥塞而不进行控制**，整个网络的**吞吐量将随输入负荷的增大而下降**。

![1](https://jsl1997.oss-cn-beijing.aliyuncs.com/note/1.png)

当输入的负载到达一定程度 吞吐量不会增加，即一部分网络资源会丢失掉，网络的吞吐量维持在其所能控制的最大值，转发节点的缓存不够大这造成分组的丢失是拥塞的征兆。
TCP的四种拥塞控制算法

1. 慢开始
2. 拥塞控制
3. 快重传
4. 快恢复

假定：

1. 数据是单方向传送，而另一个方向只传送确认
2. 接收方总是有足够大的缓存空间，因而发送发发送窗口的大小由网络的拥塞程度来决定
3. 以TCP报文段的个数为讨论问题的单位，而不是以字节为单位

![2](https://jsl1997.oss-cn-beijing.aliyuncs.com/note/2.png)

示例如下：
传输轮次：发送方给接收方发送数据报文段后，接收方给发送方发回相应的确认报文段，一个传输轮次所经历的时间就是往返时间RTT(RTT并非是恒定的数值），使用传输轮次是为了强调，把拥塞窗口cwnd所允许发送的报文段都连续发送出去，并收到了对已发送的最后一个报文段的确认，拥塞窗口cwnd会随着网络拥塞程度以及所使用的拥塞控制算法动态变化。

在tcp双方建立逻辑链接关系时， 拥塞窗口cwnd的值被设置为1，还需设置慢开始门限ssthresh,在执行慢开始算法时，发送方每收到一个对新报文段的确认时，就把拥塞窗口cwnd的值加一，然后开始下一轮的传输，当拥塞窗口cwnd增长到慢开始门限值时，就使用拥塞避免算法。

**慢开始**：
假设当前发送方拥塞窗口cwnd的值为1，而发送窗口swnd等于拥塞窗口cwnd，因此发送方当前只能发送一个数据报文段（拥塞窗口cwnd的值是几，就能发送几个数据报文段），接收方收到该数据报文段后，给发送方回复一个确认报文段，发送方收到该确认报文后，将拥塞窗口的值变为2，

> 发送方此时可以连续发送两个数据报文段，接收方收到该数据报文段后，给发送方一次发回2个确认报文段，发送方收到这两个确认报文后，将拥塞窗口的值加2变为4，发送方此时可连续发送4个报文段，接收方收到4个报文段后，给发送方依次回复4个确认报文，发送方收到确认报文后，将拥塞窗口加4，置为8，发送方此时可以连续发送8个数据报文段，接收方收到该8个数据报文段后，给发送方一次发回8个确认报文段，发送方收到这8个确认报文后，将拥塞窗口的值加8变为16，

当前的拥塞窗口cwnd的值已经等于慢开始门限值，之后改用拥塞避免算法。

**拥塞避免**：
也就是每个传输轮次，拥塞窗口cwnd只能线性加一，而不是像慢开始算法时，每个传输轮次，拥塞窗口cwnd按指数增长。同理，16+1……直至到达24，假设24个报文段在传输过程中丢失4个，接收方只收到20个报文段，给发送方依次回复20个确认报文段，一段时间后，丢失的4个报文段的重传计时器超时了，发送发判断可能出现拥塞，更改cwnd和ssthresh.并重新开始慢开始算法，如图所示：

![3](https://jsl1997.oss-cn-beijing.aliyuncs.com/note/3.png)

![4](https://jsl1997.oss-cn-beijing.aliyuncs.com/note/4.png)

**快速重传**：
发送方发送1号数据报文段，接收方收到1号报文段后给发送方发回对1号报文段的确认，在1号报文段到达发送方之前，发送方还可以将发送窗口内的2号数据报文段发送出去，接收方收到2号报文段后给发送方发回对2号报文段的确认，在2号报文段到达发送方之前，发送方还可以将发送窗口内的3号数据报文段发送出去，

> 假设该报文丢失，发送方便不会发送针对该报文的确认报文给发送方，发送方还可以将发送窗口内的4号数据报文段发送出去，接收方收到后，发现这不是按序到达的报文段，因此给发送方发送针对2号报文段的重复确认，表明我现在希望收到的是3号报文段，但是我没有收到3号报文段，而收到了未按序到达的报文段，发送方还可以将发送窗口中的5号报文段发送出去,接收方收到后，发现这不是按序到达的报文段，因此给发送方发送针对2号报文段的重复确认，表明我现在希望收到的是3号报文段，但是我没有收到3号报文段，而收到了未按序到达的报文段,，发送方还可以将发送窗口内的最后一个数据段即6号数据报文段发送出去，接收方收到后，发现这不是按序到达的报文段，因此给发送方发送针对2号报文段的重复确认，表明我现在希望收到的是3号报文段，但是我没有收到3号报文段，而收到了未按序到达的报文段，

此时，发送方收到了累计3个连续的针对2号报文段的重复确认，立即重传3号报文段，接收方收到后，给发送方发回针对6号报文的确认，表明，序号到6为至的报文都收到了，这样就不会造成发送方对3号报文的超时重传，而是提早收到了重传。

![5](https://jsl1997.oss-cn-beijing.aliyuncs.com/note/5.png)

![6](https://jsl1997.oss-cn-beijing.aliyuncs.com/note/6.png)

# TCP是如何保证可靠传输的?

TCP协议保证数据传输可靠性的方式主要有：

- 数据分片：发送端对数据进行分片，接受端要对数据进行重组，由TCP确定分片的大小并控制分片和重组

- 数据校验：TCP将保持它首部和数据的检验和，这是一个端到端的检验和，目的是检测数据在传输过程中的任何变化。如果收到分片的检验或有差错，TCP将丢弃这个分片，并不确认收到此报文段导致对端超时并重发
- 序列号：
- 确认应答：接收端接收到分片数据时，根据分片数据序号向发送端发送一个确认
- 超时重传：发送方在发送分片时设置超时定时器，如果在定时器超时之后没有收到相应的确认，重发分片数据
- 连接管理
- 流量控制（滑动窗口）：TCP连接的每一方的接受缓冲空间大小固定，接收端只允许另一端发送接收端缓冲区所能接纳的数据，TCP在滑动窗口的基础上提供流量控制，防止较快主机致使较慢主机的缓冲区溢出
- 拥塞控制

- 失序处理：作为IP数据报来传输的TCP分片到达时可能会失序，TCP将对收到的数据进行重新排序，将收到的数据以正确的顺序交给应用层；

- **重复处理**：作为IP数据报来传输的TCP分片会发生重复，TCP的接收端必须丢弃重复的数据；

# 如何理解TCP的keep-alive的原理？

在客户端和服务端间的网络一切正常、且双方都没主动发起关闭连接的请求时，此TCP连接理论上可以永久保持。但是，网络情况是及其复杂的，**在双方长时间未通讯时，如何得知对方还活着？如何得知这个TCP连接是健康且具有通讯能力的？**

HTTP协议运行在TCP协议之上，它无状态会导致客户端的每次请求都需要重新建立TCP连接，接受到服务端响应后，断开TCP连接。对于每次建立、断开TCP连接，还是有相当的性能损耗的。**那么，如何才能尽可能的减少性能损耗呢？**

正如上面提出的问题：**在双方长时间未通讯时，如何得知对方还活着？如何得知这个TCP连接是健康且具有通讯能力的？**

TCP的保活机制就是用来解决此类问题，这个机制我们也可以称作：keepalive。保活机制默认是关闭的，TCP连接的任何一方都可打开此功能。有三个主要配置参数用来控制保活功能。

如果在一段时间（**保活时间：tcp_keepalive_time**）内此连接都不活跃，开启保活功能的一端会向对端发送一个保活探测报文。

- 若对端正常存活，且连接有效，对端必然能收到探测报文并进行响应。此时，发送端收到响应报文则证明TCP连接正常，重置保活时间计数器即可。
- 若由于网络原因或其他原因导致，发送端无法正常收到保活探测报文的响应。那么在一定**探测时间间隔（tcp_keepalive_intvl）**后，将继续发送保活探测报文。直到收到对端的响应，或者达到配置的**探测循环次数上限（tcp_keepalive_probes）**都没有收到对端响应，这时对端会被认为不可达，TCP连接随存在但已失效，需要将连接做中断处理。

在探测过程中，对端主机会处于以下四种状态之一：

![v2-837ba2a1eb7beb10c036ca468f7db69f_720w](https://jsl1997.oss-cn-beijing.aliyuncs.com/note/v2-837ba2a1eb7beb10c036ca468f7db69f_720w.jpg)

TCP keepalive指的是TCP保活计时器（keepalive timer）。设想有这样的情况：客户已主动与服务器建立了TCP连接。但后来客户端的主机突然出故障。显然，服务器以后就不能再收到客户发来的数据。因此，应当有措施使服务器不要再白白等待下去。这就是使用保活计时器。服务器每收到一次客户的数据，就重新设置保活计时器，时间的设置通常是两小时。若两小时没有收到客户的数据，服务器就发送一个探测报文段，以后则每隔75秒发送一次。若一连发送10个**探测报文段**后仍无客户的响应，服务器就认为客户端出了故障，接着就关闭这个连接。

它的作用就是探测对端的连接有没有失效，通过定时发送探测包来探测连接的对端是否存活，不过默认情况下需要7200s没有数据包交互才会发送keepalive探测包，往往这个时间太久了，我们孰知的很多组件都没有开启keepalive特性，而是选择在应用层做心跳机制。

# tcp数据包最大为什么是65495字节？

IP数据报首部中的总长度字段指首部和数据之和的长度，有16位 

- IP数据报最大长度：2^16-1=65535字节 

- TCP数据段最大长度=IP数据报最大长度-IP数据报固定首部-TCP报文段固定首部 

​                                                     =65535-20-20 

​                                                    =65495字节

# TCP在listen时的参数backlog的意义？

![793763-20161106230135861-97545939](https://jsl1997.oss-cn-beijing.aliyuncs.com/note/793763-20161106230135861-97545939.png)

可见，listen状态是服务器接收连接建立的必经之路。调用listen后，服务器即进入了LISTEN状态。

listen为：

```
int listen(int sockfd, int backlog);
```

其backlog是一个建议值，用于指定内部的队列大小，以控制同时建立的连接请求数量。

针对控制连接这个需求，有两种方法实现这个backlog：

1. 单一队列来控制连接。队列中既包含了SYN_RCVD的状态，也包含了ESTABLISHED状态。accept只处理后面一种状态。如果三次握手中的ACK到来，则会在队列中直接改其状态。显然，这时backlog为这一队列的长度。
2. 两个单独队列来控制。两种状态分别实现单独的队列。显然这种情况下，两个队列都必须有明确的大小限制，backlog只能限制其中一个。

在UNP中，这个值是这样描述的：

内核维护两个队列，一个是未完成队列，一个是已建立连接的队列。在第一个sync到达时，先将连接塞入第一个队列，再回复ACK+SYNC。此时，连接状态变为SYN_RCVD；

第二个是完成队列。客户端的ACK上来后，对应的连接移入完成队列。此状态的连接会被accept系统调用返回，状态变为ESTABLISHED。

另一方面，如果请求上来时队列已满，则TCP忽略之。客户端来重试。

**可见，其实现实际是第一种单队列的形式，即backlog控制两种状态连接数的总和。**

# Nagle 算法

试想一个场景，发送端不停地给接收端发很小的包，一次只发 1 个字节，那么发 1 千个字节需要发 1000 次。这种频繁的发送是存在问题的，不光是传输的时延消耗，发送和确认本身也是需要耗时的，频繁的发送接收带来了巨大的时延。

而避免小包的频繁发送，这就是 Nagle 算法要做的事情。

具体来说，Nagle 算法的规则如下:

- 当第一次发送数据时不用等待，就算是 1byte 的小包也立即发送
- 后面发送满足下面条件之一就可以发了:
- 数据包大小达到最大段大小(Max Segment Size, 即 MSS)
- 之前所有包的 ACK 都已接收到

# 简述 TCP 协议的延迟 ACK 和累计应答

试想这样一个场景，当我收到了发送端的一个包，然后在极短的时间内又接收到了第二个包，那我是一个个地回复，还是稍微等一下，把两个包的 ACK 合并后一起回复呢？

**ACK定义**
TCP协议中，接收方成功接收到数据后，会回复一个ACK数据包，表示已经确认接收到ACK确认号前面的所有数据。
ACK字段长度为32位，能表示0~2^32-1之间的值。

**ACK作用**
发送方在一定时间内没有收到服务端的ACK确认包后，就会重新发送TCP数据包。发送方收到了ACK，表明接收方已经接收到数据，保证了数据的可靠达到。

**ACK机制**
接收方在接收到数据后，不是立即会给发送方发送ACK的。这可能由以下原因导致：

1. 收到数据包的序号前面还有需要接收的数据包。因为发送方发送数据时，并不是需要等上次发送数据被Ack就可以继续发送TCP包，而这些TCP数据包达到的顺序是不保证的，这样接收方可能先接收到后发送的TCP包（注意提交给应用层时是保证顺序的）。
2. 为了降低网络流量，ACK有延迟确认机制。
3. ACK的值到达最大值后，又会从0开始。

**ACK延迟确认机制**
接收方在收到数据后，并不会立即回复ACK，而是延迟一定时间。一般ACK延迟发送的时间为200ms，但这个200ms并非收到数据后需要延迟的时间。系统有一个固定的定时器每隔200ms会来检查是否需要发送ACK包。这样做有两个目的。

1. 这样做的目的是ACK是可以合并的，也就是指如果连续收到两个TCP包，并不一定需要ACK两次，只要回复最终的ACK就可以了，可以降低网络流量。
2. 如果接收方有数据要发送，那么就会在发送数据的TCP数据包里，带上ACK信息。这样做，可以避免大量的ACK以一个单独的TCP包发送，减少了网络流量。
   

# TIME_WAIT 2MSL

1. 为什么TIME_WAIT状态需要经过2MSL（最大报文段生存时间）才能返回到CLOSE状态？
2. MSL的大小一般为多少，可否配置？（MSL的大小一般为30，可以进行修改）

![image-20220220220445767](https://jsl1997.oss-cn-beijing.aliyuncs.com/note/image-20220220220445767.png)

如果最后一次客户端发给服务端的ACK确认报文丢失，服务端则会重传（回到第三步）。

- 对于服务端来说，在发送完数据后（即第三次挥手），在经过2MSL时间后，没有收到ACK确认，则表示报文可能丢失，就会重传。
- 对于客户端来说，在收到服务端发送的数据（即第三次挥手），就会回复确认ACK，并等待2MSL，如果在这2MSL间没有再收到来自服务端的数据，则表明，服务端没有发生重传，即正确收到了ACK回复（即第四次挥手）。

> 客户端要保证第四个数据包能被服务器收到，怎么保证呢？就是通过等待 2MSL ，第四个包传递到服务器需要 1 MSL，如果中间丢失了，服务器就会再发送第五个数据包去让客户端重新发第四个包，这第五个包传递到客户端又需要 1MSL 时间。所以客户端为了确保第四个数据包能被服务器收到，就要等待 2MSL 时间。

> 2MSL就是一一个发送和一个回复所需的最大时间。如果直到2MSL，Client都没有再次收到FIN，那么Client推断ACK已经被成功接收，则结束TCP连接。

# TCP 挥手时出现大量 CLOSE_WAIT 或 TIME_WAIT 怎么解决？

**出现大量TIME_WAIT的原因以及解决办法**

存在大量短连接，并且由该方主动关闭。一般出现在web服务器中，因为HTTP关闭TCP连接的是Server端。
HTTP1.0默认使用非持久连接，一个request和response后立马关闭TCP连接（为了实现client到web-server能支持长连接，必须在HTTP请求头里显示指定 Connection:keep-alive）；
HTTP1.1默认使用持久连接，也就是会重用TCP连接传输多个 request/response（要关闭keep-alive需要在HTTP请求头里显示指定 Connection:close)

解决办法：

![image-20220222145857947](https://jsl1997.oss-cn-beijing.aliyuncs.com/note/image-20220222145857947.png)

**出现大量CLOSE_WAIT的原因以及解决办法**

如果一直保持在CLOSE_WAIT状态，那么只有一种情况，就是在对方关闭连接之后服务器程序自己没有进一步发出ack信号。换句话说，就是在对方连接关闭之后，程序里没有检测到，或者程序压根就忘记了这个时候需要关闭连接，于是这个资源就一直被程序占着。这种情况通过服务器内核参数也没办法解决，服务器对于程序抢占的资源没有主动回收的权利，除非终止程序运行。
所以如果将大量CLOSE_WAIT的解决办法总结为一句话那就是：查代码。因为问题出在程序里头啊。

# 什么是 TCP 粘包和拆包？

对于什么是粘包、拆包问题，我想先举两个简单的应用场景：

1. 客户端和服务器建立一个连接，客户端发送一条消息，客户端关闭与服务端的连接。
2. 客户端和服务器简历一个连接，客户端连续发送两条消息，客户端关闭与服务端的连接。

对于第一种情况，服务端的处理流程可以是这样的：当客户端与服务端的连接建立成功之后，服务端不断读取客户端发送过来的数据，当客户端与服务端连接断开之后，服务端知道已经读完了一条消息，然后进行解码和后续处理...。对于第二种情况，如果按照上面相同的处理逻辑来处理，那就有问题了，我们来看看第二种情况下客户端发送的两条消息递交到服务端有可能出现的情况：

第一种情况：

服务端一共读到两个数据包，第一个包包含客户端发出的第一条消息的完整信息，第二个包包含客户端发出的第二条消息，那这种情况比较好处理，服务器只需要简单的从网络缓冲区去读就好了，第一次读到第一条消息的完整信息，消费完再从网络缓冲区将第二条完整消息读出来消费。

![839956-20170303140906032-720068642](https://jsl1997.oss-cn-beijing.aliyuncs.com/note/839956-20170303140906032-720068642.png)

第二种情况：

服务端一共就读到一个数据包，这个数据包包含客户端发出的两条消息的完整信息，这个时候基于之前逻辑实现的服务端就蒙了，因为服务端不知道第一条消息从哪儿结束和第二条消息从哪儿开始，这种情况其实是发生了TCP粘包。

![839956-20170303141040391-1377466796](https://jsl1997.oss-cn-beijing.aliyuncs.com/note/839956-20170303141040391-1377466796.png)

第三种情况：

服务端一共收到了两个数据包，第一个数据包只包含了第一条消息的一部分，第一条消息的后半部分和第二条消息都在第二个数据包中，或者是第一个数据包包含了第一条消息的完整信息和第二条消息的一部分信息，第二个数据包包含了第二条消息的剩下部分，这种情况其实是发送了TCP拆，因为发生了一条消息被拆分在两个包里面发送了，同样上面的服务器逻辑对于这种情况是不好处理的。

![839956-20170303141209985-617401891](https://jsl1997.oss-cn-beijing.aliyuncs.com/note/839956-20170303141209985-617401891.png)

**发生TCP粘包、拆包主要是由于下面一些原因：**

1. 要发送的数据小于TCP发送缓冲区的大小，TCP将多次写入缓冲区的数据一次发送出去；
2. 接收数据端的应用层没有及时读取接收缓冲区中的数据；
3. 数据发送过快，数据包堆积导致缓冲区积压多个数据后才一次性发送出去(如果客户端每发送一条数据就睡眠一段时间就不会发生粘包)；
4. 出现粘包的原因其实很复杂，涉及到TCP的多个特性，关于TCP更深入的解释可以参考我的另一篇文章:

**如何解决拆包粘包：**

既然知道了tcp是无界的数据流，且协议本身无法避免粘包，拆包的发生，那我们只能在应用层数据协议上，加以控制。通常在制定传输数据时，可以使用如下方法：

1. 使用带消息头的协议、消息头存储消息开始标识及消息长度信息，服务端获取消息头的时候解析出消息长度，然后向后读取该长度的内容。
2. 设置定长消息，服务端每次读取既定长度的内容作为一条完整消息。
3. 设置消息边界，服务端从网络流中按消息编辑分离出消息内容。

# TCP 长连接和短连接有什么不同的使用场景？

**短连接：**

>连接->传输数据->关闭连接
>比如HTTP是无状态的的短链接,浏览器和服务器每进行一-次HTTP操作， 就建立-次连接 ,但任务结束就中断连接。
>因为连接后接收了数据就断开了，所以每次数据接受处理不会有联系。这也是HTTP协议无状态的原因之一 。

**长连接：**

> 连接->传输数据->保持连接-> 传输数据-> .........直到一 方关闭连接，多是客户端关闭连接。
> 长连接指建立S0CKET连接后不管是否使用都保持连接，但安全性较差。

**什么时候用长连接，短连接？**

>长连接多用于操作频繁，点对点的通讯，而且连接数不能太多情况。每个TCP连接都需要三步握手，
>这需要时间，如果每个操作都是先连接,再操作的话那么处理速度会降低很多，所以每个操作完后都
>不断开，次处理时直接发送数据包就0K了，不用建立TCP连接。例如：数据库的连接用长连接，如果
>用短连接频繁的通信会造成socket错误，而且频繁的socket创建也是对资源的浪费。

> 而像WEB网站的http服务-般都用短链接，因为长连接对于服务端来说会耗费一定的资源，而像WEB网
> 站这么频繁的成千上万甚至上亿客户端的连接用短连接会更省一些资源， 如果用长连接,而且同时有成
> 千上万的用户，如果每个用户都占用一个连接的话,那可想而知吧。所以并发量大，但每个用户无需频
> 繁操作情况下需用短连好。

